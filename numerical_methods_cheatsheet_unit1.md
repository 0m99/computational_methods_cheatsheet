# ğŸ¯ NUMERICAL METHODS COMPLETE CHEATSHEET
## UNIT 1: Foundations, Root Finding & Optimization

---

# ğŸ“š PART A: MATHEMATICAL FOUNDATIONS

---

## 1. TAYLOR SERIES

### ğŸ§  Intuition First (Think Like a 15-Year-Old)

Imagine you're trying to describe a curvy road to a friend who can only understand straight lines. The Taylor series is like giving them better and better descriptions:

1. **First attempt**: "The road is at height 5" (just a point - constant)
2. **Better**: "The road is at height 5, going uphill" (straight line - linear)
3. **Even better**: "The road is at height 5, going uphill, and curving left" (parabola - quadratic)
4. **More terms**: Each additional term captures more curvature details!

**Real-World Analogy**: Think of it like zooming in on Google Maps. At first, a mountain looks like a point. Zoom in, it's a triangle. Zoom more, you see the actual curves. Taylor series is mathematical "zooming in."

### ğŸ“ The Mathematical Definition

The Taylor series of a function f(x) about a point x = a is:

```
f(x) = f(a) + f'(a)(x-a) + f''(a)(x-a)Â²/2! + f'''(a)(x-a)Â³/3! + ...
```

**General Formula:**
```
f(x) = Î£ [fâ½â¿â¾(a)/n!] Ã— (x-a)â¿  for n = 0 to âˆ
```

**Special Case - Maclaurin Series (when a = 0):**
```
f(x) = f(0) + f'(0)x + f''(0)xÂ²/2! + f'''(0)xÂ³/3! + ...
```

### ğŸ“ Step-by-Step Derivation

**Why does this formula work?**

1. We want to approximate f(x) as a polynomial: P(x) = câ‚€ + câ‚(x-a) + câ‚‚(x-a)Â² + ...
2. At x = a: P(a) = câ‚€ = f(a) âœ“
3. Take first derivative: P'(x) = câ‚ + 2câ‚‚(x-a) + 3câ‚ƒ(x-a)Â² + ...
   - At x = a: P'(a) = câ‚ = f'(a) âœ“
4. Take second derivative: P''(x) = 2câ‚‚ + 6câ‚ƒ(x-a) + ...
   - At x = a: P''(a) = 2câ‚‚ = f''(a), so câ‚‚ = f''(a)/2! âœ“
5. Pattern continues: câ‚™ = fâ½â¿â¾(a)/n!

### ğŸ”¥ Important Taylor Series to MEMORIZE

| Function | Taylor Series (around 0) | Convergence |
|----------|-------------------------|-------------|
| eË£ | 1 + x + xÂ²/2! + xÂ³/3! + ... | All x |
| sin(x) | x - xÂ³/3! + xâµ/5! - xâ·/7! + ... | All x |
| cos(x) | 1 - xÂ²/2! + xâ´/4! - xâ¶/6! + ... | All x |
| ln(1+x) | x - xÂ²/2 + xÂ³/3 - xâ´/4 + ... | -1 < x â‰¤ 1 |
| 1/(1-x) | 1 + x + xÂ² + xÂ³ + ... | |x| < 1 |
| (1+x)â¿ | 1 + nx + n(n-1)xÂ²/2! + ... | |x| < 1 |

### âš ï¸ Taylor's Theorem with Remainder

**CRITICAL FOR EXAMS!**

When we truncate the series at n terms, there's an ERROR. Taylor's theorem tells us:

```
f(x) = Pâ‚™(x) + Râ‚™(x)
```

Where the remainder (Lagrange form):
```
Râ‚™(x) = fâ½â¿âºÂ¹â¾(Î¾) Ã— (x-a)â¿âºÂ¹ / (n+1)!
```
where Î¾ is some point between a and x.

**Error Bound:**
```
|Râ‚™(x)| â‰¤ M Ã— |x-a|â¿âºÂ¹ / (n+1)!
```
where M = max|fâ½â¿âºÂ¹â¾(t)| for t between a and x.

### ğŸ’¡ Memory Trick
**"FIND THE DERIVATIVE, PLUG THE POINT, DIVIDE BY FACTORIAL"**
- F: Find all derivatives
- P: Plug in the center point a
- D: Divide by n!

### ğŸ“ Exam Tips
1. Always check if the series converges at your point!
2. The more terms you use, the better the approximation (usually)
3. Works best when x is close to a
4. Look for patterns in derivatives (sin, cos cycle every 4)

---

## 2. ROLLE'S THEOREM

### ğŸ§  Intuition First

Imagine throwing a ball in the air. It goes up, reaches a peak, comes down. At the peak, the ball is momentarily stationary - velocity is ZERO!

Rolle's Theorem says: If you start and end at the same height (on a smooth curve), somewhere in between, the curve must be flat (horizontal tangent).

### ğŸ“ Formal Statement

**If:**
1. f(x) is continuous on closed interval [a, b]
2. f(x) is differentiable on open interval (a, b)
3. f(a) = f(b)

**Then:**
There exists at least one point c âˆˆ (a, b) such that f'(c) = 0

### ğŸ¨ Visual Understanding

```
        f'(c) = 0 (horizontal tangent)
            â†“
           âˆ©âˆ©âˆ©
          /    \
         /      \
        /        \
    f(a)          f(b)  â† Same height!
    â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—
    a      c       b
```

### âš ï¸ All Three Conditions Are NECESSARY!

**Counterexamples:**

1. **Not continuous**: f(x) = 1/x on [-1, 1] with f(0) undefined
2. **Not differentiable**: f(x) = |x| on [-1, 1] - sharp corner at 0
3. **f(a) â‰  f(b)**: f(x) = x on [0, 1] - no horizontal tangent

### ğŸ’¡ Memory Trick
**"SAME START, SAME END, SOMEWHERE FLAT"** 
Or think of a roller coaster that starts and ends at the same height - must have a peak or valley!

---

## 3. MEAN VALUE THEOREM (MVT)

### ğŸ§  Intuition First

Imagine driving from City A to City B, 100 km apart, in 2 hours. Your average speed is 50 km/h. MVT says: At some point during your trip, your speedometer showed EXACTLY 50 km/h!

### ğŸ“ Formal Statement

**If:**
1. f(x) is continuous on [a, b]
2. f(x) is differentiable on (a, b)

**Then:**
There exists at least one c âˆˆ (a, b) such that:
```
f'(c) = [f(b) - f(a)] / (b - a)
```

### ğŸ¨ Visual Understanding

```
                    Tangent at c (same slope as secant)
                   /
                  /
        â—â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â—
       /       c         \
      /                   \
     â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—  â† Secant line (average slope)
     a                     b
```

The tangent line at some point c is PARALLEL to the secant line connecting endpoints!

### ğŸ”— Relationship to Rolle's Theorem

MVT is a GENERALIZATION of Rolle's Theorem!

If f(a) = f(b), then:
- Secant slope = [f(b) - f(a)]/(b-a) = 0
- So f'(c) = 0 â†’ Rolle's Theorem!

### ğŸ“ Important Applications

1. **Proving inequalities**: Show that |sin(x) - sin(y)| â‰¤ |x - y|
2. **Showing functions are constant**: If f'(x) = 0 everywhere, f is constant
3. **Error analysis in numerical methods**

### ğŸ’¡ Memory Trick
**"Average = Instant Somewhere"**
The AVERAGE rate of change equals the INSTANTANEOUS rate somewhere!

---

# ğŸ“š PART B: ERRORS AND COMPUTER ARITHMETIC

---

## 4. APPROXIMATIONS AND ERRORS

### ğŸ§  Intuition First

Every measurement has some error. When you say you're "5 feet 10 inches tall," you might actually be 5'10.3" - the 0.3" is an error. In numerical computing, we can't represent most numbers exactly, so errors creep in.

### ğŸ“ Types of Errors

#### 1. Absolute Error
```
Absolute Error = |True Value - Approximate Value|
                = |x - xÌƒ|
```

**Example**: True value = Ï€ = 3.14159..., Approximate = 3.14
- Absolute Error = |3.14159... - 3.14| â‰ˆ 0.00159

#### 2. Relative Error
```
Relative Error = |True Value - Approximate Value| / |True Value|
               = |x - xÌƒ| / |x|
```

**Example**: Same as above
- Relative Error = 0.00159 / 3.14159 â‰ˆ 0.0005 = 0.05%

#### 3. Percentage Error
```
Percentage Error = Relative Error Ã— 100%
```

### ğŸ”¥ Why Relative Error Matters More

**Scenario 1**: Error of 1 meter when measuring Earth-Sun distance (150 billion meters)
- Relative error â‰ˆ 0.0000000001% â†’ Negligible!

**Scenario 2**: Error of 1 meter when measuring your height (1.7 meters)
- Relative error â‰ˆ 59% â†’ Completely wrong!

Same absolute error, VASTLY different significance!

### ğŸ“ Sources of Error

| Error Type | Description | Example |
|------------|-------------|---------|
| **Inherent Error** | Error in input data | Measuring with a faulty ruler |
| **Truncation Error** | Cutting off infinite series | Using only 3 terms of Taylor series |
| **Round-off Error** | Limited decimal places | 1/3 = 0.333... stored as 0.33 |
| **Human Error** | Mistakes in calculation | Pressing wrong button |

### âš¡ Significant Figures

**Definition**: Digits that carry meaningful information

**Rules:**
1. All non-zero digits are significant: 123 has 3 sig figs
2. Zeros between non-zeros are significant: 1003 has 4 sig figs
3. Leading zeros are NOT significant: 0.0052 has 2 sig figs
4. Trailing zeros after decimal ARE significant: 2.50 has 3 sig figs

**Relationship to Error:**
If x = 3.14159 is rounded to xÌƒ = 3.14 (3 sig figs), then:
```
|x - xÌƒ| < 0.5 Ã— 10â»Â² = 0.005
```

### ğŸ“ Exam Formula: Significant Figures and Error

A number xÌƒ approximates x to n significant figures if:
```
|x - xÌƒ| / |x| < 0.5 Ã— 10â»â½â¿â»Â¹â¾
```

---

## 5. DATA REPRESENTATION AND COMPUTER ARITHMETIC

### ğŸ§  Intuition First

Computers are like people who can only count using a limited alphabet. Imagine if you could only use digits 0-9 and had exactly 10 boxes to write a number. You'd face problems with very large numbers (don't fit) or very precise decimals (not enough boxes).

### ğŸ“ Floating-Point Representation (IEEE 754)

A number is stored as:
```
x = Â± m Ã— báµ‰
```

Where:
- **Â±** = Sign (1 bit: 0 for positive, 1 for negative)
- **m** = Mantissa/Significand (fractional part, normalized)
- **b** = Base (usually 2 for computers)
- **e** = Exponent

**IEEE 754 Single Precision (32-bit):**
```
| 1 bit | 8 bits   | 23 bits   |
| Sign  | Exponent | Mantissa  |
```

**IEEE 754 Double Precision (64-bit):**
```
| 1 bit | 11 bits  | 52 bits   |
| Sign  | Exponent | Mantissa  |
```

### ğŸ”¢ Example: Storing 6.75 in Binary

1. Convert to binary: 6.75â‚â‚€ = 110.11â‚‚
2. Normalize: 110.11 = 1.1011 Ã— 2Â²
3. Store:
   - Sign: 0 (positive)
   - Exponent: 2 + 127 (bias) = 129 = 10000001â‚‚
   - Mantissa: 1011 (the 1 before decimal is implicit!)

### âš ï¸ Machine Epsilon (Îµ_mach)

**Definition**: The smallest number such that 1 + Îµ â‰  1 in computer arithmetic

```
Single Precision: Îµ_mach â‰ˆ 1.2 Ã— 10â»â·
Double Precision: Îµ_mach â‰ˆ 2.2 Ã— 10â»Â¹â¶
```

**Why it matters**: Any number smaller than Îµ_mach times your number will be "invisible" when added!

### ğŸ“ Overflow and Underflow

**Overflow**: Number too large to represent
- Single precision max: â‰ˆ 3.4 Ã— 10Â³â¸
- Result: âˆ (infinity)

**Underflow**: Number too small (close to zero)
- Single precision min: â‰ˆ 1.2 Ã— 10â»Â³â¸
- Result: 0 (dangerous - silent error!)

---

## 6. LOSS OF SIGNIFICANCE (CATASTROPHIC CANCELLATION)

### ğŸ§  Intuition First

Imagine measuring two buildings that are almost the same height:
- Building A: 100.523 meters (measured to 6 sig figs)
- Building B: 100.519 meters (measured to 6 sig figs)
- Difference: 0.004 meters (only 1 sig fig!)

You started with 6 significant figures but ended with only 1! Three is the "catastrophic cancellation."

### ğŸ“ The Mathematical Problem

When subtracting nearly equal numbers:
```
x = 1.23456789
y = 1.23456788
x - y = 0.00000001
```

If we only have 8 decimal places of precision:
- x stored as: 1.2345679 (rounded)
- y stored as: 1.2345679 (rounded)  
- Computed x - y = 0 (COMPLETELY WRONG!)

The true answer (0.00000001) is lost!

### ğŸ”¥ Classic Example: Quadratic Formula

For axÂ² + bx + c = 0, the standard formula is:
```
x = (-b Â± âˆš(bÂ² - 4ac)) / 2a
```

**Problem**: When bÂ² >> 4ac, we have âˆš(bÂ² - 4ac) â‰ˆ |b|

For the root with +âˆš when b > 0 (or -âˆš when b < 0):
```
x = (-b + âˆš(bÂ² - 4ac)) / 2a â‰ˆ (-b + b) / 2a â‰ˆ 0/2a
```
This is catastrophic cancellation!

**Solution - Rationalized Form**:
```
xâ‚ = (-b - sign(b)âˆš(bÂ² - 4ac)) / 2a    (Stable for this root)
xâ‚‚ = c / (a Ã— xâ‚)                        (Use Vieta's formula)
```

### ğŸ“ Other Examples of Loss of Significance

| Expression | Problem | Stable Alternative |
|------------|---------|-------------------|
| âˆš(x+1) - âˆšx for large x | Cancellation | 1/(âˆš(x+1) + âˆšx) |
| 1 - cos(x) for small x | cos(x) â‰ˆ 1 | 2sinÂ²(x/2) |
| eË£ - 1 for small x | eË£ â‰ˆ 1 | Use Taylor: x + xÂ²/2 + ... |
| ln(x+1) - ln(x) for large x | Nearly equal | ln(1 + 1/x) |

### ğŸ’¡ Memory Trick
**"When numbers are close, subtraction hurts the most!"**

### ğŸ“ Exam Tips
1. Watch for subtractions of nearly equal quantities
2. Look for algebraic or series alternatives
3. Check if the problem involves small or large parameters that would cause near-equality

---

# ğŸ“š PART C: ROOT FINDING METHODS

---

## 7. BISECTION METHOD

### ğŸ§  Intuition First

Imagine you're playing a number guessing game. Someone thinks of a number between 1 and 100. Each time you guess, they say "higher" or "lower." The smartest strategy? Always guess the MIDDLE number! Each guess cuts your search range in half.

The Bisection method does exactly this for finding where a function crosses zero.

### ğŸ“ The Algorithm

**Prerequisites:**
- Continuous function f(x) on [a, b]
- f(a) and f(b) have OPPOSITE signs (one positive, one negative)
- This guarantees at least one root in (a, b) by **Intermediate Value Theorem**

**Algorithm:**
```
Input: f, a, b, tolerance Îµ
Output: Approximate root r

1. While (b - a) > Îµ:
   a. Compute midpoint: c = (a + b) / 2
   b. If f(c) = 0, return c (exact root found!)
   c. If f(a) Ã— f(c) < 0:  (root is in left half)
         b = c
   d. Else:                (root is in right half)
         a = c
2. Return (a + b) / 2
```

### ğŸ¨ Visual Understanding

```
Initial: f(a) < 0        f(b) > 0
         â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—
         a       c       b
                 â†“
         Check f(c):
         
If f(c) > 0:     If f(c) < 0:
â—â”€â”€â”€â”€â”€â”€â”€â—        â—â”€â”€â”€â”€â”€â”€â”€â—
a   c=new b      a=c     b
```

### ğŸ“Š Worked Example

Find the root of f(x) = xÂ³ - x - 2 in [1, 2]

| Iteration | a | b | c = (a+b)/2 | f(a) | f(c) | f(b) | New interval |
|-----------|---|---|-------------|------|------|------|--------------|
| 1 | 1 | 2 | 1.5 | -2 | -0.125 | 4 | [1.5, 2] |
| 2 | 1.5 | 2 | 1.75 | -0.125 | 1.609 | 4 | [1.5, 1.75] |
| 3 | 1.5 | 1.75 | 1.625 | -0.125 | 0.666 | 1.609 | [1.5, 1.625] |
| 4 | 1.5 | 1.625 | 1.5625 | -0.125 | 0.252 | 0.666 | [1.5, 1.5625] |
| ... | ... | ... | ... | ... | ... | ... | ... |

Root â‰ˆ 1.5214 (actual: 1.52138...)

### ğŸ“ Convergence Analysis

**Error after n iterations:**
```
|eâ‚™| = |r - câ‚™| â‰¤ (b - a) / 2â¿âºÂ¹
```

**Number of iterations needed for tolerance Îµ:**
```
n â‰¥ logâ‚‚((b - a) / Îµ) - 1
```

Or equivalently:
```
n â‰¥ [log(b-a) - log(Îµ)] / log(2)
```

**Convergence Rate**: LINEAR with rate 1/2
- Each iteration HALVES the error
- We gain approximately logâ‚â‚€(2) â‰ˆ 0.301 decimal digits per iteration
- Need about 3.3 iterations per additional correct decimal digit

### âœ… Advantages
1. **Always converges** (if initial conditions met)
2. **Simple** to implement and understand
3. **Robust** - works for any continuous function
4. **Error bound known** in advance

### âŒ Disadvantages
1. **Slow** compared to other methods
2. **Requires bracketing interval** [a, b] with sign change
3. **Cannot find multiple roots** in one run
4. **Cannot find roots where f touches but doesn't cross zero**

### ğŸ’» Implementation (Pseudocode)

```python
def bisection(f, a, b, tol=1e-6, max_iter=100):
    if f(a) * f(b) >= 0:
        raise ValueError("f(a) and f(b) must have opposite signs")
    
    for i in range(max_iter):
        c = (a + b) / 2
        
        if abs(f(c)) < tol or (b - a) / 2 < tol:
            return c
        
        if f(a) * f(c) < 0:
            b = c
        else:
            a = c
    
    return c
```

### ğŸ’¡ Memory Trick
**"Bisection = Binary Search for Roots"**

### âš ï¸ Edge Cases
1. **Multiple roots**: Only finds one root in the interval
2. **Even-multiplicity roots**: f(x) = xÂ² touches zero but doesn't cross - Bisection FAILS!
3. **Discontinuities**: May converge to a discontinuity if it looks like a sign change

---

## 8. NEWTON-RAPHSON METHOD

### ğŸ§  Intuition First

Imagine you're on a hilly landscape trying to find sea level (y = 0). At your current position, you look at the slope of the ground and say "If I follow this slope as a straight line, where would I hit sea level?" You walk there, then repeat.

Newton's method uses the TANGENT LINE to predict where the root is.

### ğŸ“ Derivation

At point (xâ‚™, f(xâ‚™)), the tangent line has:
- Slope: f'(xâ‚™)  
- Equation: y - f(xâ‚™) = f'(xâ‚™)(x - xâ‚™)

Setting y = 0 to find x-intercept:
```
0 - f(xâ‚™) = f'(xâ‚™)(x - xâ‚™)
-f(xâ‚™) = f'(xâ‚™)(x - xâ‚™)
x = xâ‚™ - f(xâ‚™)/f'(xâ‚™)
```

**The Newton-Raphson Formula:**
```
xâ‚™â‚Šâ‚ = xâ‚™ - f(xâ‚™) / f'(xâ‚™)
```

### ğŸ¨ Visual Understanding

```
      Tangent line at xâ‚™
          \
           \
    f(xâ‚™) â—â”€\
            \
             \
    â”€â”€â”€â”€â”€â—â”€â”€â”€â”€\â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ y = 0
        xâ‚™â‚Šâ‚   xâ‚™
         â†‘
    New approximation!
```

### ğŸ“Š Worked Example

Find âˆš2 (i.e., solve xÂ² - 2 = 0) starting with xâ‚€ = 1

f(x) = xÂ² - 2,  f'(x) = 2x

Formula: xâ‚™â‚Šâ‚ = xâ‚™ - (xâ‚™Â² - 2)/(2xâ‚™) = (xâ‚™ + 2/xâ‚™)/2

| n | xâ‚™ | f(xâ‚™) | f'(xâ‚™) | xâ‚™â‚Šâ‚ |
|---|-----|-------|--------|------|
| 0 | 1 | -1 | 2 | 1.5 |
| 1 | 1.5 | 0.25 | 3 | 1.4167 |
| 2 | 1.4167 | 0.00694 | 2.833 | 1.41422 |
| 3 | 1.41422 | 0.00001 | 2.828 | 1.414213562 |

âˆš2 = 1.41421356... - we got 9 correct digits in just 4 iterations!

### ğŸ“ Convergence Analysis

**Taylor Series Analysis:**

Let r be the true root and eâ‚™ = xâ‚™ - r be the error.

Expanding f(r) = 0 around xâ‚™:
```
0 = f(xâ‚™) + f'(xâ‚™)(r - xâ‚™) + f''(Î¾)(r - xâ‚™)Â²/2
0 = f(xâ‚™) - f'(xâ‚™)eâ‚™ + f''(Î¾)eâ‚™Â²/2
```

From Newton's formula: xâ‚™â‚Šâ‚ = xâ‚™ - f(xâ‚™)/f'(xâ‚™)
```
eâ‚™â‚Šâ‚ = xâ‚™â‚Šâ‚ - r = xâ‚™ - f(xâ‚™)/f'(xâ‚™) - r = eâ‚™ - f(xâ‚™)/f'(xâ‚™)
```

After algebraic manipulation:
```
eâ‚™â‚Šâ‚ = [f''(Î¾) / (2f'(xâ‚™))] Ã— eâ‚™Â²
```

**This means:**
```
eâ‚™â‚Šâ‚ â‰ˆ C Ã— eâ‚™Â²
```

where C = f''(r) / (2f'(r)) near the root.

**Convergence Rate**: QUADRATIC (order 2)
- The number of correct digits approximately DOUBLES each iteration!
- If you have 2 correct digits, next iteration gives ~4, then ~8, then ~16...

### âš ï¸ Convergence Conditions

Newton's method converges if:
1. **f, f', f'' exist and are continuous** near the root
2. **f'(r) â‰  0** (simple root)
3. **Starting point xâ‚€ is "close enough"** to the root

**Sufficient Condition (Newton-Kantorovich):**
```
|f(xâ‚€) Ã— f''(x)| / |f'(xâ‚€)|Â² < 1
```
for all x in the neighborhood of xâ‚€.

### âŒ Failure Cases

1. **f'(xâ‚™) = 0**: Division by zero! Method crashes.
   - Example: f(x) = xÂ³ at x = 0 has f'(0) = 0

2. **Cycles/Divergence**: Can oscillate or diverge
   - Example: f(x) = xÂ³ - 2x + 2 with xâ‚€ = 0 â†’ xâ‚ = 1 â†’ xâ‚‚ = 0 â†’ ...

3. **Multiple roots**: Converges LINEARLY, not quadratically
   - For root of multiplicity m: xâ‚™â‚Šâ‚ = xâ‚™ - mÂ·f(xâ‚™)/f'(xâ‚™) (modified Newton)

4. **Wrong basin**: May converge to a different root than intended

### ğŸ’» Implementation

```python
def newton(f, df, x0, tol=1e-10, max_iter=100):
    x = x0
    for i in range(max_iter):
        fx = f(x)
        dfx = df(x)
        
        if abs(dfx) < 1e-14:  # Avoid division by zero
            raise ValueError("Derivative too small")
        
        x_new = x - fx / dfx
        
        if abs(x_new - x) < tol:
            return x_new
        
        x = x_new
    
    raise ValueError("Did not converge")
```

### ğŸ“Š Computational Cost

Per iteration:
- 1 function evaluation: f(xâ‚™)
- 1 derivative evaluation: f'(xâ‚™)
- 1 division

**Total per iteration: 2 function evaluations** (counting derivative separately)

### ğŸ’¡ Memory Tricks

1. **"Newton = Tangent Chasing"**
2. **Formula**: "x_new = x - f/f' " (fraction of function over slope)
3. **Convergence**: "Quadratic = Digits DOUBLE"

### ğŸ“ Exam Tips

1. Always state the formula clearly
2. Show the derivative calculation
3. Track iterations in a table
4. Mention quadratic convergence
5. Note when it might fail (f' = 0, bad starting point)

---

## 9. SECANT METHOD

### ğŸ§  Intuition First

Newton's method is great, but it needs the derivative f'(x). What if computing the derivative is hard or expensive?

The Secant method says: "Approximate the derivative using two points!" Instead of the tangent (exact slope), use a secant line (approximate slope through two points).

### ğŸ“ Derivation

Replace f'(xâ‚™) in Newton's formula with a finite difference approximation:
```
f'(xâ‚™) â‰ˆ [f(xâ‚™) - f(xâ‚™â‚‹â‚)] / [xâ‚™ - xâ‚™â‚‹â‚]
```

Substituting into Newton's formula:
```
xâ‚™â‚Šâ‚ = xâ‚™ - f(xâ‚™) / {[f(xâ‚™) - f(xâ‚™â‚‹â‚)] / [xâ‚™ - xâ‚™â‚‹â‚]}
     = xâ‚™ - f(xâ‚™)(xâ‚™ - xâ‚™â‚‹â‚) / [f(xâ‚™) - f(xâ‚™â‚‹â‚)]
```

**The Secant Formula:**
```
xâ‚™â‚Šâ‚ = xâ‚™ - f(xâ‚™) Ã— (xâ‚™ - xâ‚™â‚‹â‚) / [f(xâ‚™) - f(xâ‚™â‚‹â‚)]
```

Or equivalently:
```
xâ‚™â‚Šâ‚ = [xâ‚™â‚‹â‚f(xâ‚™) - xâ‚™f(xâ‚™â‚‹â‚)] / [f(xâ‚™) - f(xâ‚™â‚‹â‚)]
```

### ğŸ¨ Visual Understanding

```
                Secant line through (xâ‚™â‚‹â‚, f(xâ‚™â‚‹â‚)) and (xâ‚™, f(xâ‚™))
                            \
    f(xâ‚™â‚‹â‚) â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\
                              \
    f(xâ‚™)           â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\
                                \
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â”€â”€\â—â”€â”€â”€â”€â”€ y = 0
            xâ‚™â‚Šâ‚     xâ‚™        xâ‚™â‚‹â‚
```

### ğŸ“Š Worked Example

Find root of f(x) = xÂ² - 2 with xâ‚€ = 1, xâ‚ = 2

| n | xâ‚™â‚‹â‚ | xâ‚™ | f(xâ‚™â‚‹â‚) | f(xâ‚™) | xâ‚™â‚Šâ‚ |
|---|------|-----|---------|-------|------|
| 1 | 1 | 2 | -1 | 2 | 1.333 |
| 2 | 2 | 1.333 | 2 | -0.222 | 1.400 |
| 3 | 1.333 | 1.400 | -0.222 | -0.040 | 1.4146 |
| 4 | 1.400 | 1.4146 | -0.040 | 0.00119 | 1.41421 |

Root â‰ˆ 1.41421 (âˆš2 = 1.41421356...)

### ğŸ“ Convergence Analysis

**Convergence Order**: Ï† = (1 + âˆš5)/2 â‰ˆ 1.618 (the Golden Ratio!)

```
eâ‚™â‚Šâ‚ â‰ˆ C Ã— eâ‚™ Ã— eâ‚™â‚‹â‚
```

Which leads to:
```
eâ‚™â‚Šâ‚ â‰ˆ C' Ã— |eâ‚™|^Ï†
```

**This means:**
- Faster than linear (Bisection)
- Slower than quadratic (Newton)
- "Superlinear" convergence

### ğŸ“Š Comparison Table

| Method | Order | Function Evals per Iter | Derivatives Needed |
|--------|-------|------------------------|-------------------|
| Bisection | 1 (linear) | 1 | No |
| Secant | 1.618 | 1 | No |
| Newton | 2 (quadratic) | 2 | Yes (f') |

**Efficiency Index** = p^(1/cost) where p is convergence order:
- Bisection: 1^1 = 1
- Secant: 1.618^1 = 1.618
- Newton: 2^(1/2) = 1.414

Secant actually has the HIGHEST efficiency per function evaluation!

### âš ï¸ Conditions and Limitations

**Requires:**
1. Two initial guesses xâ‚€ and xâ‚
2. f continuous
3. xâ‚€ and xâ‚ "close enough" to the root

**Can fail when:**
1. f(xâ‚™) = f(xâ‚™â‚‹â‚) (horizontal secant - division by zero)
2. Poor initial guesses
3. Multiple or complex roots nearby

### ğŸ’» Implementation

```python
def secant(f, x0, x1, tol=1e-10, max_iter=100):
    for i in range(max_iter):
        f0, f1 = f(x0), f(x1)
        
        if abs(f1 - f0) < 1e-14:
            raise ValueError("Division by near-zero")
        
        x2 = x1 - f1 * (x1 - x0) / (f1 - f0)
        
        if abs(x2 - x1) < tol:
            return x2
        
        x0, x1 = x1, x2
    
    raise ValueError("Did not converge")
```

### ğŸ’¡ Memory Tricks

1. **"Secant = Newton without derivative"**
2. **"Two points make a secant; one point needs a tangent"**
3. **"Golden ratio convergence for Gold-standard efficiency"**

### ğŸ“ Exam Tips

1. Remember you need TWO initial points
2. The formula looks complicated - derive it from Newton by approximating f'
3. Mention the golden ratio convergence order
4. Know that it's more efficient per function evaluation than Newton

---

# ğŸ“š PART D: OPTIMIZATION METHODS

---

## 10. FIBONACCI SEARCH (1-D Minimization)

### ğŸ§  Intuition First

You're searching for the lowest point in a valley, but you can only "sample" the height at certain points. How do you narrow down the search efficiently?

Fibonacci search is like a sophisticated version of bisection for finding the MINIMUM of a function. It uses the Fibonacci sequence to decide where to place evaluation points optimally.

### ğŸ“ The Setup

**Given:**
- Unimodal function f(x) on interval [a, b]
- **Unimodal** means: exactly ONE minimum, function decreases then increases

**Goal:**
- Find x* where f(x*) is minimum
- Reduce interval to size â‰¤ Îµ

### ğŸ“ Fibonacci Sequence Review

```
Fâ‚ = 1, Fâ‚‚ = 1, Fâ‚ƒ = 2, Fâ‚„ = 3, Fâ‚… = 5, Fâ‚† = 8, Fâ‚‡ = 13, ...
Fâ‚™ = Fâ‚™â‚‹â‚ + Fâ‚™â‚‹â‚‚
```

### ğŸ“ The Algorithm

**Step 1**: Determine n (number of iterations) such that:
```
Fâ‚™ â‰¥ (b - a) / Îµ
```

**Step 2**: Initialize
```
xâ‚ = a + (Fâ‚™â‚‹â‚‚/Fâ‚™)(b - a)
xâ‚‚ = a + (Fâ‚™â‚‹â‚/Fâ‚™)(b - a)
```

**Step 3**: For k = 1, 2, ..., n-1:
```
If f(xâ‚) > f(xâ‚‚):
    a = xâ‚
    xâ‚ = xâ‚‚
    xâ‚‚ = a + (Fâ‚™â‚‹â‚–â‚‹â‚/Fâ‚™â‚‹â‚–)(b - a)
Else:
    b = xâ‚‚
    xâ‚‚ = xâ‚
    xâ‚ = a + (Fâ‚™â‚‹â‚–â‚‹â‚‚/Fâ‚™â‚‹â‚–)(b - a)
```

### ğŸ¨ Visual Understanding

```
f(x)
  â”‚    
  â”‚    âˆ©
  â”‚   / \
  â”‚  /   \___
  â”‚ /        \
  â”‚/          \
  â””â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â”€â—â”€â”€ x
      a  xâ‚ xâ‚‚  b
      
Compare f(xâ‚) and f(xâ‚‚):
- If f(xâ‚) > f(xâ‚‚): minimum is in [xâ‚, b]
- If f(xâ‚) < f(xâ‚‚): minimum is in [a, xâ‚‚]
```

### ğŸ“ Why Fibonacci Numbers?

The ratio Fâ‚™â‚‹â‚/Fâ‚™ approaches 1/Ï† â‰ˆ 0.618 (golden ratio inverse).

Fibonacci placement ensures:
1. After each iteration, ONE point can be reused
2. Interval reduction is optimal for a fixed number of evaluations
3. After n iterations: final interval = (b-a)/Fâ‚™

### ğŸ“Š Reduction Ratio

After n function evaluations (n-1 iterations):
```
Final interval size = (b - a) / Fâ‚™
```

For large n, this approaches:
```
(b - a) Ã— Ï†â»â¿ â‰ˆ (b - a) Ã— (0.618)â¿
```

### âœ… Advantages
1. Optimal for fixed number of function evaluations
2. No derivatives needed
3. Well-defined termination

### âŒ Disadvantages
1. Must know n (number of iterations) in advance
2. Only works for unimodal functions
3. More complex than Golden Section

---

## 11. GOLDEN SECTION SEARCH

### ğŸ§  Intuition First

Golden Section is like Fibonacci search but simpler. Instead of using Fibonacci ratios, it uses the golden ratio Ï„ = (âˆš5 - 1)/2 â‰ˆ 0.618 consistently.

Think of it as the "limiting case" of Fibonacci search as n â†’ âˆ.

### ğŸ“ The Golden Ratio

```
Ï† = (1 + âˆš5)/2 â‰ˆ 1.618 (Golden Ratio)
Ï„ = 1/Ï† = (âˆš5 - 1)/2 â‰ˆ 0.618 (Inverse Golden Ratio)
```

**Key Property**: Ï„Â² = 1 - Ï„, or equivalently, Ï„Â² + Ï„ = 1

### ğŸ“ The Algorithm

**Initialize**:
```
xâ‚ = a + (1-Ï„)(b-a) = a + 0.382(b-a)
xâ‚‚ = a + Ï„(b-a) = a + 0.618(b-a)
Evaluate f(xâ‚) and f(xâ‚‚)
```

**Iterate** until |b - a| < Îµ:
```
If f(xâ‚) > f(xâ‚‚):
    a = xâ‚
    xâ‚ = xâ‚‚
    f(xâ‚) = f(xâ‚‚)  [reuse!]
    xâ‚‚ = a + Ï„(b-a)
    Evaluate f(xâ‚‚)
Else:
    b = xâ‚‚
    xâ‚‚ = xâ‚
    f(xâ‚‚) = f(xâ‚)  [reuse!]
    xâ‚ = a + (1-Ï„)(b-a)
    Evaluate f(xâ‚)
```

### ğŸ“Š Worked Example

Minimize f(x) = (x-2)Â² on [0, 5], Ï„ = 0.618

| Iter | a | b | xâ‚ | xâ‚‚ | f(xâ‚) | f(xâ‚‚) | New interval |
|------|---|---|-----|-----|-------|-------|--------------|
| 0 | 0 | 5 | 1.91 | 3.09 | 0.008 | 1.188 | [0, 3.09] |
| 1 | 0 | 3.09 | 1.18 | 1.91 | 0.672 | 0.008 | [1.18, 3.09] |
| 2 | 1.18 | 3.09 | 1.91 | 2.36 | 0.008 | 0.130 | [1.18, 2.36] |
| ... | ... | ... | ... | ... | ... | ... | ... |

Converges to x* â‰ˆ 2 âœ“

### ğŸ“ Convergence Rate

Each iteration reduces interval by factor Ï„ â‰ˆ 0.618:
```
Interval after n iterations = (b-a) Ã— Ï„â¿
```

Number of iterations for tolerance Îµ:
```
n = log((b-a)/Îµ) / log(1/Ï„) â‰ˆ 2.078 Ã— logâ‚â‚€((b-a)/Îµ)
```

### ğŸ’» Implementation

```python
def golden_section(f, a, b, tol=1e-6):
    tau = (math.sqrt(5) - 1) / 2  # â‰ˆ 0.618
    
    x1 = a + (1 - tau) * (b - a)
    x2 = a + tau * (b - a)
    f1, f2 = f(x1), f(x2)
    
    while (b - a) > tol:
        if f1 > f2:
            a = x1
            x1, f1 = x2, f2
            x2 = a + tau * (b - a)
            f2 = f(x2)
        else:
            b = x2
            x2, f2 = x1, f1
            x1 = a + (1 - tau) * (b - a)
            f1 = f(x1)
    
    return (a + b) / 2
```

### ğŸ“Š Comparison: Fibonacci vs Golden Section

| Aspect | Fibonacci | Golden Section |
|--------|-----------|----------------|
| Ratio used | Fâ‚™â‚‹â‚/Fâ‚™ (varies) | Ï„ â‰ˆ 0.618 (constant) |
| Optimality | Optimal for fixed n | Slightly suboptimal |
| Implementation | Harder (need Fâ‚™) | Simpler |
| Number of evals | Fixed (n) | Variable (until tolerance) |

---

## 12. NEWTON'S METHOD FOR OPTIMIZATION

### ğŸ§  Intuition First

At a minimum, the derivative (slope) is zero: f'(x*) = 0. So finding a minimum is like finding the ROOT of f'(x)!

Newton's method for optimization applies Newton's root-finding method to f'(x).

### ğŸ“ Derivation

Apply Newton's root-finding to g(x) = f'(x):
```
xâ‚™â‚Šâ‚ = xâ‚™ - g(xâ‚™)/g'(xâ‚™) = xâ‚™ - f'(xâ‚™)/f''(xâ‚™)
```

**Newton's Optimization Formula:**
```
xâ‚™â‚Šâ‚ = xâ‚™ - f'(xâ‚™) / f''(xâ‚™)
```

### ğŸ“ Geometric Interpretation

We're approximating f(x) by a quadratic (Taylor expansion to 2nd order):
```
f(x) â‰ˆ f(xâ‚™) + f'(xâ‚™)(x-xâ‚™) + Â½f''(xâ‚™)(x-xâ‚™)Â²
```

The minimum of this quadratic is found by setting derivative to zero:
```
f'(xâ‚™) + f''(xâ‚™)(x-xâ‚™) = 0
x = xâ‚™ - f'(xâ‚™)/f''(xâ‚™)
```

### ğŸ“Š Worked Example

Minimize f(x) = xâ´ - 4xÂ³ + 2, starting at xâ‚€ = 4

f'(x) = 4xÂ³ - 12xÂ², f''(x) = 12xÂ² - 24x

| n | xâ‚™ | f'(xâ‚™) | f''(xâ‚™) | xâ‚™â‚Šâ‚ |
|---|-----|--------|---------|------|
| 0 | 4 | 64 | 96 | 3.333 |
| 1 | 3.333 | 16.3 | 53.3 | 3.027 |
| 2 | 3.027 | 1.98 | 37.2 | 2.974 |
| 3 | 2.974 | 0.047 | 34.5 | 2.9986 |

Minimum at x* â‰ˆ 3

### âš ï¸ Important Conditions

1. **f''(xâ‚™) â‰  0** (avoid division by zero)
2. **f''(x*) > 0** (ensures minimum, not maximum or saddle)
3. Starting point close enough to minimum

### ğŸ“ Convergence

**Quadratic convergence** (like Newton root-finding):
```
eâ‚™â‚Šâ‚ â‰ˆ C Ã— eâ‚™Â²
```

Much faster than Golden Section, but needs derivatives!

### ğŸ“Š Comparison of 1D Optimization Methods

| Method | Derivatives | Convergence | Robust |
|--------|-------------|-------------|--------|
| Fibonacci | None | Linear (â‰ˆ0.618) | Yes |
| Golden Section | None | Linear (â‰ˆ0.618) | Yes |
| Newton's | f', f'' | Quadratic | No (may diverge) |

---

## 13. STEEPEST DESCENT (Gradient Descent)

### ğŸ§  Intuition First

Imagine you're on a mountain in thick fog and want to reach the valley. You can't see far, but you CAN feel which way is downhill at your feet. The steepest descent strategy: always walk in the direction that goes downhill the fastest!

### ğŸ“ Multivariate Setup

**Function**: f(x) where x = (xâ‚, xâ‚‚, ..., xâ‚™) is a vector

**Gradient**: âˆ‡f = (âˆ‚f/âˆ‚xâ‚, âˆ‚f/âˆ‚xâ‚‚, ..., âˆ‚f/âˆ‚xâ‚™)

**Key Fact**: The gradient âˆ‡f points in the direction of STEEPEST INCREASE. So -âˆ‡f points toward STEEPEST DECREASE.

### ğŸ“ The Algorithm

```
Input: Starting point xâ‚€, tolerance Îµ

Repeat:
    1. Compute gradient: g = âˆ‡f(xâ‚–)
    2. If ||g|| < Îµ, stop (at minimum)
    3. Direction: d = -g (negative gradient)
    4. Line search: Find Î± that minimizes f(xâ‚– + Î±d)
    5. Update: xâ‚–â‚Šâ‚ = xâ‚– + Î±d
```

### ğŸ“ Line Search Methods

**Option 1: Exact Line Search**
Solve: Î±* = argmin f(xâ‚– - Î±âˆ‡f(xâ‚–))

**Option 2: Backtracking (Armijo) Line Search**
Start with Î± = 1, and while f(xâ‚– + Î±d) > f(xâ‚–) + cÂ·Î±Â·âˆ‡f(xâ‚–)áµ€d:
    Î± = ÏÂ·Î± (shrink by factor Ï â‰ˆ 0.5)

### ğŸ“Š Worked Example

Minimize f(x,y) = xÂ² + 4yÂ² starting from (4, 4)

âˆ‡f = (2x, 8y)

| k | (x,y) | âˆ‡f | Direction | Î±* | New (x,y) |
|---|-------|-----|-----------|-----|-----------|
| 0 | (4,4) | (8,32) | (-8,-32) | 0.147 | (2.82,-0.70) |
| 1 | (2.82,-0.70) | (5.65,-5.65) | (-5.65,5.65) | 0.147 | (2.0,0.12) |
| ... | ... | ... | ... | ... | ... |

Converges to (0, 0) âœ“

### ğŸ“ Convergence Analysis

For a quadratic function f(x) = Â½xáµ€Ax - báµ€x where A is positive definite:

**Convergence rate**:
```
||xâ‚–â‚Šâ‚ - x*|| â‰¤ [(Îº-1)/(Îº+1)]Â² Ã— ||xâ‚– - x*||
```

Where Îº = Î»â‚˜â‚â‚“/Î»â‚˜áµ¢â‚™ is the **condition number** of A.

**Problem**: When Îº is large (ill-conditioned), convergence is SLOW!

### ğŸ¨ Zigzag Behavior

```
     â•²    â•²    â•²
      â•²    â•²    â•²
   â†˜   â•²    â•²    â—xâ‚€
        â†˜   â•²
         â†˜  â—xâ‚
      xâ‚ƒâ—    â†˜
             â—xâ‚‚
          â—x*
```

The path "zig-zags" especially when contours are elongated ellipses!

### âœ… Advantages
1. Simple to implement
2. Works in any dimension
3. Always moves toward lower function values

### âŒ Disadvantages
1. Slow for ill-conditioned problems
2. Zig-zag behavior
3. Only finds local minima
4. Sensitive to step size

---

## 14. NELDER-MEAD (SIMPLEX) ALGORITHM

### ğŸ§  Intuition First

Imagine a bouncy, stretchy triangle (in 2D) or a shape with n+1 vertices in n dimensions. This shape "rolls downhill" toward the minimum, stretching, shrinking, and flipping as needed to fit into valleys.

It's a "derivative-free" method - you only need function values!

### ğŸ“ Key Concepts

**Simplex**: A shape with n+1 vertices in n dimensions
- 1D: Line segment (2 points)
- 2D: Triangle (3 points)
- 3D: Tetrahedron (4 points)

### ğŸ“ The Operations

Starting with vertices xâ‚, xâ‚‚, ..., xâ‚™â‚Šâ‚ sorted by function value:
- **Best**: xáµ¦ = xâ‚ (lowest f value)
- **Worst**: xáµ¥ = xâ‚™â‚Šâ‚ (highest f value)
- **Second worst**: xâ‚› = xâ‚™
- **Centroid** (excluding worst): xÌ„ = (xâ‚ + xâ‚‚ + ... + xâ‚™)/n

**Four Operations:**

1. **Reflection**: xáµ£ = xÌ„ + Î±(xÌ„ - xáµ¥), typically Î± = 1
   - "Flip" the worst point through the centroid

2. **Expansion**: xâ‚‘ = xÌ„ + Î³(xáµ£ - xÌ„), typically Î³ = 2
   - If reflection is good, try going further!

3. **Contraction**: xc = xÌ„ + Ï(xáµ¥ - xÌ„), typically Ï = 0.5
   - If reflection is bad, shrink toward centroid

4. **Shrink**: xáµ¢ = xáµ¦ + Ïƒ(xáµ¢ - xáµ¦), typically Ïƒ = 0.5
   - Shrink entire simplex toward best point

### ğŸ“ The Algorithm

```
1. Initialize simplex with n+1 points
2. Sort vertices: f(xâ‚) â‰¤ f(xâ‚‚) â‰¤ ... â‰¤ f(xâ‚™â‚Šâ‚)
3. Compute centroid xÌ„ (excluding xâ‚™â‚Šâ‚)
4. REFLECT: xáµ£ = xÌ„ + Î±(xÌ„ - xâ‚™â‚Šâ‚)
   
   If f(xâ‚) â‰¤ f(xáµ£) < f(xâ‚™):
       Replace xâ‚™â‚Šâ‚ with xáµ£
   
   Else if f(xáµ£) < f(xâ‚):  [Reflection is best so far!]
       EXPAND: xâ‚‘ = xÌ„ + Î³(xáµ£ - xÌ„)
       If f(xâ‚‘) < f(xáµ£): Replace xâ‚™â‚Šâ‚ with xâ‚‘
       Else: Replace xâ‚™â‚Šâ‚ with xáµ£
   
   Else:  [f(xáµ£) â‰¥ f(xâ‚™)]
       CONTRACT: xc = xÌ„ + Ï(xâ‚™â‚Šâ‚ - xÌ„)
       If f(xc) < f(xâ‚™â‚Šâ‚): Replace xâ‚™â‚Šâ‚ with xc
       Else: SHRINK all points toward xâ‚

5. Repeat until convergence
```

### ğŸ¨ Visual (2D Case)

```
   Operations on triangle:
   
   REFLECT         EXPAND          CONTRACT        SHRINK
      â—xáµ£            â—xâ‚‘
     /               /                               â—xâ‚
    /               /                  â—xáµ¥          / \
   â—â”€â”€â”€â—           â—â”€â”€â”€â—           â—â”€â”€â”€â—â”€â”€â—xc      â—â”€â”€â”€â—
   â”‚   â”‚           â”‚   â”‚           â”‚   â”‚           (smaller)
   â—xáµ¥             â—xáµ¥             â—               
```

### ğŸ“Š Convergence Properties

- **No formal convergence guarantee** for general functions
- Works well in practice for smooth functions
- Can stall on non-smooth or noisy functions
- Typically converges to local minimum

### âœ… Advantages
1. **No derivatives needed** - derivative-free!
2. Simple to implement
3. Works well for n â‰¤ 10 dimensions
4. Robust to noise

### âŒ Disadvantages
1. Slow for high dimensions (n > 10)
2. No convergence guarantee
3. May converge to non-stationary points
4. Sensitive to initial simplex

### ğŸ’» Simple Implementation Sketch

```python
def nelder_mead(f, x0, tol=1e-6, max_iter=1000):
    n = len(x0)
    # Initialize simplex
    simplex = [x0]
    for i in range(n):
        point = x0.copy()
        point[i] += 0.5  # Perturbation
        simplex.append(point)
    
    alpha, gamma, rho, sigma = 1, 2, 0.5, 0.5
    
    for _ in range(max_iter):
        # Sort by function values
        simplex.sort(key=f)
        
        # Check convergence
        if max([f(x) - f(simplex[0]) for x in simplex]) < tol:
            return simplex[0]
        
        # Centroid (excluding worst)
        centroid = sum(simplex[:-1]) / n
        
        # Reflection
        xr = centroid + alpha * (centroid - simplex[-1])
        
        # Apply appropriate operation based on f(xr)
        # ... (expansion, contraction, shrink logic)
    
    return simplex[0]
```

### ğŸ’¡ Memory Trick

**"RECS" - Reflect, Expand, Contract, Shrink**

Think of a jellyfish pulsing through water, changing shape to navigate!

---

# ğŸ“Š UNIT 1 SUMMARY TABLE

| Method | Type | Order | Derivatives | Robust | Best For |
|--------|------|-------|-------------|--------|----------|
| Bisection | Root | 1 | None | Yes | Safe, guaranteed |
| Newton | Root | 2 | f' | No | Fast, smooth functions |
| Secant | Root | 1.618 | None | Moderate | Fast, no derivative |
| Fibonacci | Min | Linear | None | Yes | Fixed evaluation budget |
| Golden | Min | Linear | None | Yes | Simple 1D minimization |
| Newton-Opt | Min | 2 | f', f'' | No | Smooth 1D optimization |
| Steepest | Min | Linear | âˆ‡f | Yes | Simple multi-D |
| Nelder-Mead | Min | N/A | None | Moderate | Black-box optimization |

---

# ğŸ¯ EXAM PREPARATION CHECKLIST - UNIT 1

â–¡ Can derive Taylor series for eË£, sin(x), cos(x)?
â–¡ Know Taylor remainder formula?
â–¡ Can state Rolle's and MVT with conditions?
â–¡ Understand absolute vs relative error?
â–¡ Know IEEE floating-point structure?
â–¡ Can identify catastrophic cancellation?
â–¡ Can implement Bisection step-by-step?
â–¡ Know Newton formula and derive it?
â–¡ Understand when Newton fails?
â–¡ Know Secant formula and golden ratio convergence?
â–¡ Can explain Golden Section search?
â–¡ Understand gradient descent direction?
â–¡ Know Nelder-Mead operations?
