# ğŸ¯ NUMERICAL METHODS COMPLETE CHEATSHEET
## UNIT 3: Linear Systems, Eigenvalues & Splines

---

# ğŸ“š PART A: SYSTEMS OF LINEAR EQUATIONS

---

## 1. EXISTENCE OF SOLUTIONS

### ğŸ§  Intuition First

A system of linear equations is like asking: "Where do these lines/planes/hyperplanes all meet?"

Possible outcomes:
- **One solution**: They all meet at exactly one point
- **No solution**: They never all meet (inconsistent)
- **Infinite solutions**: They overlap along a line/plane

### ğŸ“ Matrix Form

System: Ax = b

Where:
- A is the coefficient matrix (n Ã— n)
- x is the unknown vector
- b is the right-hand side

### ğŸ“ Conditions for Unique Solution

**Theorem**: Ax = b has a UNIQUE solution if and only if:
1. det(A) â‰  0 (A is non-singular/invertible)
2. rank(A) = rank([A|b]) = n

**Cases**:
| det(A) | rank(A) vs rank([A|b]) | Solutions |
|--------|------------------------|-----------|
| â‰  0 | rank = n | Unique |
| = 0 | rank(A) = rank([A|b]) < n | Infinite |
| = 0 | rank(A) < rank([A|b]) | None |

### ğŸ’¡ Memory Trick
**"DET â‰  0 â†’ One Solution, DET = 0 â†’ Check Ranks"**

---

## 2. GAUSS ELIMINATION METHOD

### ğŸ§  Intuition First

Transform the messy system into a "staircase" form where you can solve from bottom to top.

It's like organizing a pile of tangled equations into a neat pyramid!

### ğŸ“ The Process

**Phase 1: Forward Elimination** (make upper triangular)
**Phase 2: Back Substitution** (solve from bottom up)

### ğŸ“ Step-by-Step Algorithm

**Given**: Ax = b (n equations, n unknowns)

**Forward Elimination** (for k = 1 to n-1):
```
For i = k+1 to n:
    multiplier = aáµ¢â‚– / aâ‚–â‚–
    For j = k to n:
        aáµ¢â±¼ = aáµ¢â±¼ - multiplier Ã— aâ‚–â±¼
    báµ¢ = báµ¢ - multiplier Ã— bâ‚–
```

**Back Substitution**:
```
xâ‚™ = bâ‚™ / aâ‚™â‚™
For i = n-1 down to 1:
    xáµ¢ = (báµ¢ - Î£â±¼â‚Œáµ¢â‚Šâ‚â¿ aáµ¢â±¼xâ±¼) / aáµ¢áµ¢
```

### ğŸ“Š Worked Example

Solve:
```
2x + y - z = 8
-3x - y + 2z = -11
-2x + y + 2z = -3
```

**Augmented matrix**:
```
[ 2   1  -1 |  8 ]
[-3  -1   2 | -11]
[-2   1   2 | -3 ]
```

**Step 1**: Eliminate x from rows 2, 3

R2 â†’ R2 + (3/2)R1:
```
[ 2   1    -1  |   8  ]
[ 0  1/2  1/2  |  1   ]
[-2   1    2   | -3   ]
```

R3 â†’ R3 + R1:
```
[ 2   1    -1  |   8  ]
[ 0  1/2  1/2  |   1  ]
[ 0   2    1   |   5  ]
```

**Step 2**: Eliminate y from row 3

R3 â†’ R3 - 4Ã—R2:
```
[ 2   1    -1  |   8  ]
[ 0  1/2  1/2  |   1  ]
[ 0   0   -1   |   1  ]
```

**Back Substitution**:
- From R3: -z = 1 â†’ z = -1
- From R2: y/2 + (-1)/2 = 1 â†’ y = 3
- From R1: 2x + 3 - (-1) = 8 â†’ x = 2

**Solution**: x = 2, y = 3, z = -1 âœ“

### ğŸ“ Computational Cost

**Forward Elimination**:
```
Multiplications: Î£â‚–â‚Œâ‚â¿â»Â¹ (n-k)(n-k+1) â‰ˆ nÂ³/3
Additions: Same order â‰ˆ nÂ³/3
```

**Back Substitution**:
```
Multiplications: n(n+1)/2 â‰ˆ nÂ²/2
Additions: n(n-1)/2 â‰ˆ nÂ²/2
```

**Total**: O(nÂ³/3) multiplications and O(nÂ³/3) additions

Or approximately: **2nÂ³/3 operations** in total

### ğŸ’¡ Memory Trick
**"Gauss = Triangle then Climb" (make triangle, climb up solving)**

---

## 3. PIVOTING

### ğŸ§  Why Pivoting?

**Problem**: If the pivot element (diagonal) is zero or very small:
1. Zero â†’ Division by zero (crash!)
2. Very small â†’ Large round-off errors

**Solution**: Swap rows to get a better pivot!

### ğŸ“ Types of Pivoting

**Partial Pivoting** (most common):
- At step k, find the largest |aáµ¢â‚–| for i â‰¥ k
- Swap that row with row k
- Then proceed with elimination

**Complete Pivoting** (rarely used):
- Find the largest element in the entire remaining submatrix
- Swap both rows AND columns
- More accurate but expensive

**Scaled Partial Pivoting**:
- Account for the scale of each row
- Find max |aáµ¢â‚–|/sáµ¢ where sáµ¢ = max|aáµ¢â±¼| in row i
- Better for badly scaled systems

### ğŸ“Š Example: Why Pivoting Matters

Without pivoting in 3-digit arithmetic:
```
0.001x + y = 1
    x + y = 2
```

Multiplier = 1/0.001 = 1000
R2 â†’ R2 - 1000Ã—R1:
```
0.001x + y = 1
      -999y = -998
```
y = 998/999 â‰ˆ 0.999 (3 digits: 1.00)
x = (1 - 1)/0.001 = 0 (WRONG! Should be â‰ˆ 1)

With pivoting (swap rows first):
```
x + y = 2
0.001x + y = 1
```
Multiplier = 0.001
R2 â†’ R2 - 0.001Ã—R1:
```
x + y = 2
0.999y = 0.998
```
y â‰ˆ 1.00, x = 2 - 1 = 1 âœ“

---

## 4. GAUSS-JORDAN METHOD

### ğŸ§  Intuition First

Gauss-Jordan goes further than Gauss - it makes the matrix into the IDENTITY matrix, not just upper triangular. Then the solution just appears in the right-hand side!

### ğŸ“ The Process

1. Forward elimination (like Gauss) â†’ upper triangular
2. Backward elimination â†’ diagonal matrix
3. Divide each row by its diagonal â†’ identity matrix

### ğŸ“ Algorithm

Same forward elimination as Gauss, then:

**Backward Elimination** (for k = n down to 2):
```
For i = k-1 down to 1:
    multiplier = aáµ¢â‚– / aâ‚–â‚–
    For j = 1 to n:
        aáµ¢â±¼ = aáµ¢â±¼ - multiplier Ã— aâ‚–â±¼
    báµ¢ = báµ¢ - multiplier Ã— bâ‚–
```

**Normalization**:
```
For i = 1 to n:
    báµ¢ = báµ¢ / aáµ¢áµ¢
```

### ğŸ“Š Result

[A|b] â†’ [I|x]

The solution x appears directly!

### ğŸ“ Computational Cost

**Total operations**: O(nÂ³/2) - MORE than Gauss elimination!

Why use it then?
- Finding matrix inverse: [A|I] â†’ [I|Aâ»Â¹]
- Cleaner for theoretical analysis

### ğŸ’¡ Memory Trick
**"Gauss-Jordan = Complete cleanup to Identity"**

---

## 5. LU DECOMPOSITION

### ğŸ§  Intuition First

Instead of solving Ax = b directly, factor A = LU where:
- L = Lower triangular matrix (ones on diagonal)
- U = Upper triangular matrix

Then solve in two easy steps:
1. Ly = b (forward substitution)
2. Ux = y (back substitution)

**Advantage**: Once you have L and U, solving for DIFFERENT b vectors is cheap!

### ğŸ“ Doolittle Algorithm

**Condition**: A can be factored as A = LU where L has 1's on diagonal

```
For each column k:
    For i = k to n:  (compute U elements)
        uâ‚–áµ¢ = aâ‚–áµ¢ - Î£â±¼â‚Œâ‚áµâ»Â¹ lâ‚–â±¼uâ±¼áµ¢
    
    For i = k+1 to n:  (compute L elements)
        láµ¢â‚– = (aáµ¢â‚– - Î£â±¼â‚Œâ‚áµâ»Â¹ láµ¢â±¼uâ±¼â‚–) / uâ‚–â‚–
```

### ğŸ“Š Worked Example

Factor:
```
A = [2  1  1]
    [4  3  3]
    [8  7  9]
```

**Step 1**: First column of U, first column of L
```
uâ‚â‚ = 2, uâ‚â‚‚ = 1, uâ‚â‚ƒ = 1
lâ‚‚â‚ = 4/2 = 2
lâ‚ƒâ‚ = 8/2 = 4
```

**Step 2**: Second row of U, second column of L
```
uâ‚‚â‚‚ = 3 - (2)(1) = 1
uâ‚‚â‚ƒ = 3 - (2)(1) = 1
lâ‚ƒâ‚‚ = (7 - 4Ã—1)/1 = 3
```

**Step 3**: Third row of U
```
uâ‚ƒâ‚ƒ = 9 - (4)(1) - (3)(1) = 2
```

**Result**:
```
L = [1  0  0]     U = [2  1  1]
    [2  1  0]         [0  1  1]
    [4  3  1]         [0  0  2]
```

Verify: LU = A âœ“

### ğŸ“ Crout's Algorithm

**Difference**: L has the general values, U has 1's on diagonal

Same idea, slightly different formulas.

### ğŸ“ Cholesky Decomposition

**For symmetric positive definite matrices only!**

A = LLáµ€ where L is lower triangular

```
For j = 1 to n:
    lâ±¼â±¼ = âˆš(aâ±¼â±¼ - Î£â‚–â‚Œâ‚Ê²â»Â¹ lâ±¼â‚–Â²)
    
    For i = j+1 to n:
        láµ¢â±¼ = (aáµ¢â±¼ - Î£â‚–â‚Œâ‚Ê²â»Â¹ láµ¢â‚–lâ±¼â‚–) / lâ±¼â±¼
```

**Advantages**:
- Only need to store L (half the matrix!)
- About HALF the operations of LU
- Numerically very stable
- If Cholesky fails (negative under sqrt), matrix isn't positive definite

### ğŸ“ Computational Costs

| Method | Operations |
|--------|-----------|
| Gauss Elimination | nÂ³/3 |
| LU Decomposition | nÂ³/3 (same as Gauss) |
| LU Solve (given L,U) | nÂ² |
| Gauss-Jordan | nÂ³/2 |
| Cholesky | nÂ³/6 |

### ğŸ’¡ Memory Trick
**"LU = Factor Once, Solve Many Times Cheaply"**

---

# ğŸ“š PART B: EIGENVALUE PROBLEMS

---

## 6. POWER METHOD

### ğŸ§  Intuition First

Repeatedly multiply a vector by matrix A. The vector will eventually point in the direction of the "dominant" eigenvector, and the scaling factor gives the largest eigenvalue!

Like repeatedly stretching something - it naturally aligns with the strongest direction.

### ğŸ“ The Algorithm

**Goal**: Find Î»â‚ (largest eigenvalue) and vâ‚ (corresponding eigenvector)

```
1. Start with initial guess xâ½â°â¾ (random nonzero vector)
2. Repeat until convergence:
   a. yâ½áµâ¾ = A Ã— xâ½áµâ»Â¹â¾
   b. Î»â½áµâ¾ = max component of yâ½áµâ¾ (or use Rayleigh quotient)
   c. xâ½áµâ¾ = yâ½áµâ¾ / Î»â½áµâ¾ (normalize)
3. Î»â‚ â‰ˆ Î»â½áµâ¾, vâ‚ â‰ˆ xâ½áµâ¾
```

**Alternative**: Rayleigh quotient for eigenvalue:
```
Î»â½áµâ¾ = (xâ½áµâ¾)áµ€ A xâ½áµâ¾ / (xâ½áµâ¾)áµ€ xâ½áµâ¾
```

### ğŸ“Š Worked Example

Find dominant eigenvalue of:
```
A = [2  1]
    [1  2]
```

Starting with xâ½â°â¾ = [1, 1]áµ€

**Iteration 1**:
```
yâ½Â¹â¾ = A Ã— [1,1]áµ€ = [3, 3]áµ€
Î»â½Â¹â¾ = 3
xâ½Â¹â¾ = [1, 1]áµ€
```

**Iteration 2**:
```
yâ½Â²â¾ = A Ã— [1,1]áµ€ = [3, 3]áµ€
Î»â½Â²â¾ = 3
xâ½Â²â¾ = [1, 1]áµ€
```

Already converged! Î»â‚ = 3, vâ‚ = [1, 1]áµ€

(Actual eigenvalues: 3 and 1)

### ğŸ“ Convergence Analysis

Let eigenvalues be |Î»â‚| > |Î»â‚‚| â‰¥ ... â‰¥ |Î»â‚™|

**Convergence rate**: |Î»â‚‚/Î»â‚|

```
Error at step k â‰ˆ C Ã— |Î»â‚‚/Î»â‚|áµ
```

**Faster convergence when**:
- |Î»â‚| >> |Î»â‚‚| (dominant eigenvalue is much larger)

**Slow convergence when**:
- Î»â‚‚ â‰ˆ Î»â‚ (eigenvalues are close)

### âš ï¸ Conditions and Limitations

**Works when**:
1. There IS a unique dominant eigenvalue (|Î»â‚| > |Î»â‚‚|)
2. Initial vector has a component in direction of vâ‚

**Fails when**:
1. Two eigenvalues with same magnitude (e.g., Î» = Â±3)
2. Initial vector is orthogonal to vâ‚ (unlikely in practice with rounding)

### ğŸ“ Inverse Power Method

To find the SMALLEST eigenvalue:

Apply Power method to Aâ»Â¹!

**Why?** If Î» is eigenvalue of A, then 1/Î» is eigenvalue of Aâ»Â¹.
The smallest Î» of A becomes the largest eigenvalue of Aâ»Â¹.

**Shifted Inverse Power**:
To find eigenvalue closest to a guess Î¼:
Apply power method to (A - Î¼I)â»Â¹

### ğŸ’¡ Memory Trick
**"Power = Pump up the dominant direction"**

---

# ğŸ“š PART C: SPLINE FUNCTIONS

---

## 7. FIRST-DEGREE SPLINES (Linear Splines)

### ğŸ§  Intuition First

Just connect the dots with straight lines! Simple but not smooth.

### ğŸ“ Definition

Given points (xâ‚€,yâ‚€), (xâ‚,yâ‚), ..., (xâ‚™,yâ‚™):

On interval [xáµ¢, xáµ¢â‚Šâ‚]:
```
S(x) = yáµ¢ + [(yáµ¢â‚Šâ‚ - yáµ¢)/(xáµ¢â‚Šâ‚ - xáµ¢)] Ã— (x - xáµ¢)
```

### ğŸ“ Properties

- **Continuous**: Yes âœ“
- **Smooth (CÂ¹)**: No âœ— (corners at nodes)
- **Easy to compute**: Yes âœ“

### ğŸ“ Error

```
|f(x) - S(x)| â‰¤ (hÂ²/8) Ã— max|f''|
```
where h = max spacing

---

## 8. SECOND-DEGREE SPLINES (Quadratic Splines)

### ğŸ“ Definition

Piecewise quadratic polynomials:
```
Sáµ¢(x) = aáµ¢ + báµ¢(x - xáµ¢) + cáµ¢(x - xáµ¢)Â²
```

### ğŸ“ Conditions

For n+1 points (n intervals):
- Interpolation: Sáµ¢(xáµ¢) = yáµ¢ and Sáµ¢(xáµ¢â‚Šâ‚) = yáµ¢â‚Šâ‚ â†’ 2n equations
- Smoothness at interior nodes: S'áµ¢â‚‹â‚(xáµ¢) = S'áµ¢(xáµ¢) â†’ n-1 equations
- Total: 3n unknowns, 3n-1 equations

**Need 1 extra condition** (e.g., S'(xâ‚€) = 0 or given slope)

### ğŸ“ Properties

- **Continuous**: Yes âœ“
- **CÂ¹ smooth**: Yes âœ“
- **CÂ² smooth**: No âœ—

---

## 9. NATURAL CUBIC SPLINES

### ğŸ§  Intuition First

Cubic splines are like a flexible ruler (drafting spline) bent through the data points. The name literally comes from the physical tool!

A cubic spline minimizes "bending energy" - it's the smoothest way to connect points.

### ğŸ“ Definition

Piecewise cubic polynomials:
```
Sáµ¢(x) = aáµ¢ + báµ¢(x - xáµ¢) + cáµ¢(x - xáµ¢)Â² + dáµ¢(x - xáµ¢)Â³
```

### ğŸ“ Conditions

For n+1 points (n intervals), we need 4n coefficients:

1. **Interpolation** at left endpoints: Sáµ¢(xáµ¢) = yáµ¢ â†’ n equations
2. **Interpolation** at right endpoints: Sáµ¢(xáµ¢â‚Šâ‚) = yáµ¢â‚Šâ‚ â†’ n equations
3. **First derivative continuity**: S'áµ¢â‚‹â‚(xáµ¢) = S'áµ¢(xáµ¢) â†’ n-1 equations
4. **Second derivative continuity**: S''áµ¢â‚‹â‚(xáµ¢) = S''áµ¢(xáµ¢) â†’ n-1 equations

Total: 4n-2 equations for 4n unknowns

**Need 2 extra conditions!**

### ğŸ“ Natural Spline Boundary Conditions

**Natural**: S''(xâ‚€) = 0 and S''(xâ‚™) = 0

(The spline is "straight" at the endpoints - zero curvature)

### ğŸ“ Other Boundary Conditions

- **Clamped**: S'(xâ‚€) = f'(xâ‚€) and S'(xâ‚™) = f'(xâ‚™) (given slopes)
- **Not-a-knot**: Third derivative continuous at xâ‚ and xâ‚™â‚‹â‚

### ğŸ“ Algorithm for Natural Cubic Splines

**Step 1**: From conditions, derive a tridiagonal system for the second derivatives Máµ¢ = S''(xáµ¢)

Let háµ¢ = xáµ¢â‚Šâ‚ - xáµ¢

The system is:
```
háµ¢â‚‹â‚Máµ¢â‚‹â‚ + 2(háµ¢â‚‹â‚ + háµ¢)Máµ¢ + háµ¢Máµ¢â‚Šâ‚ = 6[(yáµ¢â‚Šâ‚-yáµ¢)/háµ¢ - (yáµ¢-yáµ¢â‚‹â‚)/háµ¢â‚‹â‚]
```
for i = 1, ..., n-1, with Mâ‚€ = Mâ‚™ = 0

**Step 2**: Solve the tridiagonal system (O(n) operations!)

**Step 3**: Compute coefficients:
```
aáµ¢ = yáµ¢
cáµ¢ = Máµ¢/2
dáµ¢ = (Máµ¢â‚Šâ‚ - Máµ¢)/(6háµ¢)
báµ¢ = (yáµ¢â‚Šâ‚ - yáµ¢)/háµ¢ - háµ¢(Máµ¢â‚Šâ‚ + 2Máµ¢)/6
```

### ğŸ“ Properties of Cubic Splines

1. **CÂ² continuity**: Smooth up to second derivative
2. **Minimum curvature property**: Among all CÂ² functions interpolating the data, cubic spline minimizes âˆ«[S''(x)]Â² dx
3. **Local modification**: Changing one data point only affects nearby spline pieces
4. **Stability**: Well-conditioned, robust to perturbations

### ğŸ“ Error Bound

```
|f(x) - S(x)| â‰¤ (5/384) Ã— hâ´ Ã— max|fâ½â´â¾|
```

O(hâ´) - same as Simpson's rule!

### ğŸ’¡ Memory Trick
**"Cubic Spline = Flexible ruler through points = Smooth + Natural"**

---

## 10. B-SPLINES (Basis Splines)

### ğŸ§  Intuition First

Instead of defining splines piece by piece, define special "bump" functions that you can combine. Each B-spline is like a smooth bump centered at a knot.

### ğŸ“ Definition

B-splines of degree k are defined recursively:

**Degree 0** (step function):
```
Báµ¢,â‚€(x) = 1 if táµ¢ â‰¤ x < táµ¢â‚Šâ‚, else 0
```

**Higher degrees** (Cox-de Boor recursion):
```
Báµ¢,â‚–(x) = [(x - táµ¢)/(táµ¢â‚Šâ‚– - táµ¢)]Báµ¢,â‚–â‚‹â‚(x) + [(táµ¢â‚Šâ‚–â‚Šâ‚ - x)/(táµ¢â‚Šâ‚–â‚Šâ‚ - táµ¢â‚Šâ‚)]Báµ¢â‚Šâ‚,â‚–â‚‹â‚(x)
```

### ğŸ“ Properties of B-Splines

1. **Local support**: Báµ¢,â‚–(x) is nonzero only for x âˆˆ [táµ¢, táµ¢â‚Šâ‚–â‚Šâ‚]
2. **Non-negativity**: Báµ¢,â‚–(x) â‰¥ 0
3. **Partition of unity**: Î£Báµ¢,â‚–(x) = 1 for x in the proper range
4. **Smoothness**: Cáµâ»Â¹ continuous
5. **Recursion**: Built from lower-degree B-splines

### ğŸ“ Spline as B-Spline Combination

Any spline of degree k can be written as:
```
S(x) = Î£áµ¢ cáµ¢ Báµ¢,â‚–(x)
```

The coefficients cáµ¢ are called **control points**.

### ğŸ“ Advantages of B-Splines

1. **Numerical stability**: Better conditioned than polynomial basis
2. **Local control**: Changing one cáµ¢ only affects local region
3. **Efficient evaluation**: De Boor's algorithm is O(kÂ²)
4. **Widely used**: Computer graphics, CAD, data fitting

---

## 11. INTERPOLATION VS APPROXIMATION

### ğŸ“ Key Distinction

| Interpolation | Approximation |
|---------------|---------------|
| Pass through ALL data points | Get "close" to data points |
| Exact at nodes | Minimizes overall error |
| Can oscillate wildly | Typically smoother |
| Sensitive to noise | More robust to noise |

### ğŸ“ Least Squares Approximation

For noisy data, fit a simpler curve that minimizes:
```
E = Î£áµ¢ [yáµ¢ - P(xáµ¢)]Â²
```

**Linear least squares** (fitting a line):
```
y = a + bx

b = [nÎ£xáµ¢yáµ¢ - Î£xáµ¢Î£yáµ¢] / [nÎ£xáµ¢Â² - (Î£xáµ¢)Â²]
a = (Î£yáµ¢ - bÎ£xáµ¢) / n
```

### ğŸ’¡ Memory Trick
**"Interpolation = Through the points; Approximation = Near the points"**

---

# ğŸ“Š UNIT 3 SUMMARY TABLE

| Method | Purpose | Complexity | Key Feature |
|--------|---------|------------|-------------|
| Gauss Elimination | Solve Ax=b | O(nÂ³/3) | Upper triangular |
| Gauss-Jordan | Solve Ax=b / Inverse | O(nÂ³/2) | Identity matrix |
| LU (Doolittle/Crout) | Factor A=LU | O(nÂ³/3) | Reuse for multiple b |
| Cholesky | Factor A=LLáµ€ | O(nÂ³/6) | Symmetric positive definite |
| Power Method | Largest eigenvalue | O(nÂ² per iter) | Simple, iterative |
| Linear Spline | Interpolation | O(n) | Simple, not smooth |
| Cubic Spline | Interpolation | O(n) | CÂ² smooth, natural |
| B-Spline | Interpolation/Approx | O(kÂ²) | Local control |

---

# ğŸ¯ EXAM CHECKLIST - UNIT 3

â–¡ Can perform Gauss elimination with back substitution?
â–¡ Know when/why to use pivoting?
â–¡ Understand LU decomposition concept?
â–¡ Can derive Cholesky for 2Ã—2 matrix?
â–¡ Know the Power method algorithm?
â–¡ Understand convergence rate of Power method?
â–¡ Know properties of natural cubic splines?
â–¡ Can state boundary conditions for natural splines?
â–¡ Understand difference between interpolation and approximation?
